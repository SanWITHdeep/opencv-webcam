<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Facial Expression Recognition</title>
    <style>
        body {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif;
            display: flex;
            flex-direction: column;
            align-items: center;
            justify-content: center;
            height: 100vh;
            margin: 0;
            background-color: #f0f2f5;
            color: #333;
        }
        #container {
            position: relative;
            display: flex;
            flex-direction: column;
            align-items: center;
            border: 1px solid #ddd;
            border-radius: 12px;
            padding: 24px;
            background-color: #ffffff;
            box-shadow: 0 6px 18px rgba(0,0,0,0.1);
        }
        video {
            border-radius: 8px;
            transform: scaleX(-1); /* Mirror the video for a more natural feel */
            border: 1px solid #ccc;
        }
        #emotionImage {
            margin-top: 20px;
            width: 128px;
            height: 128px;
            border-radius: 50%;
            border: 4px solid #007bff;
            transition: all 0.2s ease-in-out;
        }
        #emotionImage:hover {
            transform: scale(1.1);
        }
        #loadingMessage {
            position: absolute;
            top: 45%;
            left: 50%;
            transform: translate(-50%, -50%);
            background: rgba(0,0,0,0.75);
            color: white;
            padding: 20px;
            border-radius: 10px;
            font-size: 1.2em;
            z-index: 10;
        }
    </style>
</head>
<body>

    <h2>Facial Expression Detector üòÄüòêüòØüòü</h2>
    <div id="container">
        <div id="loadingMessage">Loading AI Models... Please Wait.</div>
        <video id="video" width="640" height="480" autoplay muted></video>
        <img id="emotionImage" src="neutral.png" alt="Current Emotion">
    </div>

    <script defer src="https://cdn.jsdelivr.net/npm/face-api.js@0.22.2/dist/face-api.min.js"></script>
    
    <script>
        const video = document.getElementById('video');
        const emotionImage = document.getElementById('emotionImage');
        const loadingMessage = document.getElementById('loadingMessage');
        const MODEL_URL = 'https://cdn.jsdelivr.net/npm/face-api.js@0.22.2/weights';

        // Function to start the camera
        async function startVideo() {
            try {
                const stream = await navigator.mediaDevices.getUserMedia({ video: true });
                video.srcObject = stream;
            } catch (err) {
                console.error("Error accessing the camera: ", err);
                loadingMessage.innerText = "Camera access denied. Please allow camera access and refresh.";
            }
        }

        // Function to load the AI models from the CDN
        async function loadModels() {
            try {
                await Promise.all([
                    faceapi.nets.tinyFaceDetector.loadFromUri(MODEL_URL),
                    faceapi.nets.faceLandmark68Net.loadFromUri(MODEL_URL),
                    faceapi.nets.faceExpressionNet.loadFromUri(MODEL_URL)
                ]);
                startVideo();
            } catch (err) {
                console.error("Error loading models: ", err);
                loadingMessage.innerText = "Could not load AI models. Check connection and refresh.";
            }
        }

        loadModels();

        // Event listener for when the video starts playing
        video.addEventListener('play', () => {
            loadingMessage.style.display = 'none'; // Hide loading message

            // Run detection every 200 milliseconds
            setInterval(async () => {
                const detections = await faceapi.detectSingleFace(video, new faceapi.TinyFaceDetectorOptions())
                    .withFaceLandmarks()
                    .withFaceExpressions();

                if (detections && detections.expressions) {
                    const expressions = detections.expressions;
                    let primaryEmotion = 'neutral';
                    let maxConfidence = 0;

                    // Find the expression with the highest confidence
                    for (const [emotion, confidence] of Object.entries(expressions)) {
                        if (confidence > maxConfidence) {
                            maxConfidence = confidence;
                            primaryEmotion = emotion;
                        }
                    }

                    // Update the image source based on the detected emotion
                    // The image files must be named 'happy.png', 'sad.png', etc.
                    emotionImage.src = `${primaryEmotion}.png`;
                    emotionImage.alt = primaryEmotion;
                }
            }, 200); // Check for expressions every 200ms
        });
    </script>

</body>
</html>
